from diffusion4med.models.diffusion import (
    FPN3d,
    HeadFPN3d,
    WeightStandardizedFPN3d,
    WeightStandardizedHeadFPN3d,
    Diffusion,
    CosineSheduler,
    LinearAttention,
    QuadraticAttention,
)
import torch.nn.functional as F
from torch.utils.data import DataLoader, RandomSampler
from thunder.callbacks import TimeProfiler, MetricLogger
from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor
from lightning.pytorch.loggers import WandbLogger
from thunder.policy import Switch
from thunder.placeholders import GroupName
from diffusion4med.utils import ValVisualization
from lightning import Trainer
from diffusion4med.data.pretrain.public import ModifiedPublic
from thunder.layout import SingleSplit
from lightning.pytorch.strategies.deepspeed import DeepSpeedStrategy
from vox2vec.config import (
    PretrainColorAugmentationsConfig,
    PublicPretrainDataRootsConfig,
    PretrainSpatialAugmentationsConfig,
    PretrainMaskingConfig,
)
from efficient_training.memory_reduction import reduce_memory_footprint
from diffusion4med.models.diffusion.blocks import ResBlock

BATCH_SIZE = 4
NUM_BATCHES_PER_EPOCH = 100
NUM_WORKERS = 10
VAL_RATIO = 0.001
RANDOM_STATE = 43
PATCH_SIZE = (128, 128, 32)
SPACING = 1.0, 1.0, 2.0
# PRE_TIME = [10, 30, 60]
strategy = "ddp_find_unused_parameters_true"

color_augmentations = PretrainColorAugmentationsConfig(blur_or_sharpen_p=0, noise_p=0)

pretrain_dataset = ModifiedPublic(
    data_roots=PublicPretrainDataRootsConfig(),
    spatial_augmentations=PretrainSpatialAugmentationsConfig(
        patch_size=PATCH_SIZE,
        max_spacing=SPACING,
        min_spacing=SPACING,
        max_patch_aspect_ratio=1,
    ),
    color_augmentations=color_augmentations,
    masking=PretrainMaskingConfig(ratio=0),
    max_num_voxels_per_patch=0,
    batch_size=BATCH_SIZE,
    cache_root=None,
)
split_dataset = SingleSplit(
    pretrain_dataset, val=VAL_RATIO, train=1 - VAL_RATIO, random_state=RANDOM_STATE
)
pretrain_sampler = RandomSampler(
    split_dataset.train,
    replacement=True,
    num_samples=NUM_BATCHES_PER_EPOCH * BATCH_SIZE,
)
train_data = DataLoader(
    split_dataset.train,
    batch_size=None,
    sampler=pretrain_sampler,
    num_workers=NUM_WORKERS,
    prefetch_factor=16,
)
val_data = DataLoader(split_dataset.val, batch_size=None, num_workers=NUM_WORKERS)


timesteps = 300
channels = (32, 64, 128, 256, 512, 1024)
in_channels = 1
num_log_images = 10
slice_visualize = PATCH_SIZE[-1] // 2
image_shape = (1, *PATCH_SIZE)
num_blocks = ((1, 1), (2, 2), (4, 4), (6, 6), (8, 8))
attention_types = (
    (LinearAttention, LinearAttention),
    (LinearAttention, LinearAttention),
    (LinearAttention, LinearAttention),
    (LinearAttention, LinearAttention),
    (QuadraticAttention, QuadraticAttention),
)

loss = F.smooth_l1_loss
backbone = FPN3d(
    in_channels=in_channels,
    timesteps=timesteps,
    channels=channels,
    num_blocks=num_blocks,
    attention_types=attention_types,
)
wrapped_backbone = reduce_memory_footprint(backbone, ResBlock, core_operation_names_iterable=('convolution', 'group_norm'))
head = HeadFPN3d(in_channels=channels[0], out_channels=in_channels)
module = Diffusion(
    backbone=wrapped_backbone,
    head=head,
    timesteps=timesteps,
    scheduler=CosineSheduler,
    num_log_images=num_log_images,
    slice_visualize=slice_visualize,
    criterion=loss,
    lr=1e-4,
    lr_scheduler=Switch({0: 1e-4, 1000: 1e-5, 3500: 1e-6}),
    image_shape=image_shape,
    # pre_time=PRE_TIME,
    # log_time_step=PRE_TIME[1]
)

wandb_logger = WandbLogger(name=GroupName, project="Public")

trainer = Trainer(
    callbacks=[
        ModelCheckpoint(save_top_k=1, every_n_epochs=1000),
        TimeProfiler(),
        LearningRateMonitor("epoch"),
        MetricLogger(single_metrics={"loss": loss}),
        # ValVisualization(),
    ],
    accelerator="gpu",
    precision="bf16-mixed",
    devices=1,
    max_epochs=10000,
    limit_val_batches=min(10, len(split_dataset.val)),
    check_val_every_n_epoch=20,
    logger=wandb_logger,
    num_sanity_val_steps=0,
    # strategy=strategy,
)
